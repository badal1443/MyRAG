{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec32e2d-e6c8-44f7-8c99-a6610d9e0fc4",
   "metadata": {},
   "source": [
    "## First stage \n",
    "### Downloaded the sample stackoverflow data from hugging face with 13224 records.\n",
    "#### The URL for dataset : hf://datasets/mlfoundations-dev/stackexchange_devops/data/train-00000-of-00001.parquet\n",
    "> **Following steps are taken:**\n",
    "> - Create Virtual environment , in this case I used python 3.12\n",
    "> - From top right select kernel 3.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec62345f-f4ac-4af0-be67-b226075899fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in ./myenv3.12/lib/python3.12/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv3.12/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-macosx_12_0_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_10_9_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./myenv3.12/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./myenv3.12/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in ./myenv3.12/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./myenv3.12/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in ./myenv3.12/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./myenv3.12/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv3.12/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv3.12/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-macosx_12_0_x86_64.whl (32.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.1/32.1 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-macosx_10_9_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-macosx_10_9_x86_64.whl (31 kB)\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, fsspec, dill, pandas, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "Successfully installed datasets-3.3.2 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 pandas-2.2.3 pyarrow-19.0.1 pytz-2025.1 tzdata-2025.1 xxhash-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/badalsingh/Workspace/LLMs/LLMOps/myenv3.12/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ba8be9-1381-4d6f-bd1e-e79bf69f6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990d4a88-95ef-4238-9d59-afe687d2b506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13224"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/mlfoundations-dev/stackexchange_devops/data/train-00000-of-00001.parquet\")\n",
    "df.head(5)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a644a975-1f3f-450d-84ec-2c238b408855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>completion</th>\n",
       "      <th>conversations</th>\n",
       "      <th>myspec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My build process packages my application in a ...</td>\n",
       "      <td>Yes, your deployment revision can use a `.nupk...</td>\n",
       "      <td>[{'from': 'human', 'value': 'My build process ...</td>\n",
       "      <td>Question: My build process packages my applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So here is your job role:\\n\\nYou help in desig...</td>\n",
       "      <td>Based on the job role you've described, a good...</td>\n",
       "      <td>[{'from': 'human', 'value': 'So here is your j...</td>\n",
       "      <td>Question: So here is your job role:\\n\\nYou hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon S3 has an option of cross-region replic...</td>\n",
       "      <td>Amazon S3's cross-region replication (CRR) is ...</td>\n",
       "      <td>[{'from': 'human', 'value': 'Amazon S3 has an ...</td>\n",
       "      <td>Question: Amazon S3 has an option of cross-reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've had some very interesting conversations t...</td>\n",
       "      <td>In a DevOps environment, where cross-functiona...</td>\n",
       "      <td>[{'from': 'human', 'value': 'I've had some ver...</td>\n",
       "      <td>Question: I've had some very interesting conve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  My build process packages my application in a ...   \n",
       "1  So here is your job role:\\n\\nYou help in desig...   \n",
       "2  Amazon S3 has an option of cross-region replic...   \n",
       "3  I've had some very interesting conversations t...   \n",
       "\n",
       "                                          completion  \\\n",
       "0  Yes, your deployment revision can use a `.nupk...   \n",
       "1  Based on the job role you've described, a good...   \n",
       "2  Amazon S3's cross-region replication (CRR) is ...   \n",
       "3  In a DevOps environment, where cross-functiona...   \n",
       "\n",
       "                                       conversations  \\\n",
       "0  [{'from': 'human', 'value': 'My build process ...   \n",
       "1  [{'from': 'human', 'value': 'So here is your j...   \n",
       "2  [{'from': 'human', 'value': 'Amazon S3 has an ...   \n",
       "3  [{'from': 'human', 'value': 'I've had some ver...   \n",
       "\n",
       "                                              myspec  \n",
       "0  Question: My build process packages my applica...  \n",
       "1  Question: So here is your job role:\\n\\nYou hel...  \n",
       "2  Question: Amazon S3 has an option of cross-reg...  \n",
       "3  Question: I've had some very interesting conve...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new column combining both questions and answers\n",
    "df[\"myspec\"]=\"Question: \"+df[\"instruction\"]+\" Answer:  \" + df[\"completion\"]\n",
    "# check first 4 rows for 3 colums in dataframe df\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d65d59d-5624-4e44-b884-0c382bc3a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing my script, I am minimizing the token cost to embedding model and just using 50 records from dataframe.\n",
    "## These fifty records will be written into a json file\n",
    "df50=df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8259ddc4-a13b-4a49-9a48-f4fa6ae0f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write this dataframe df new created column \"myspec\" into a json file.\n",
    "## File name : devops_data.json\n",
    "\n",
    "json_data=df50[\"myspec\"].to_json(orient='records')\n",
    "with open (\"devops_data50.json\",\"w\") as devops_data:\n",
    "    devops_data.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa56a17-722a-420c-86ec-ff8d3c5a742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"devops_data50.json\",\"r\") as data_f50:\n",
    "    data_50=json.load(data_f50)\n",
    "    print(len(data_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a39e6-819f-4a4b-a333-a339b959a1cc",
   "metadata": {},
   "source": [
    "## 2nd stage:\n",
    "#### Load JSON document with 50 records only, to not exceed quota of embedding model\n",
    "#### Use the json file created with only 50 records of \"myspec\" column from dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8fc8b3-245a-43e3-8a90-2d7c2bdf25cc",
   "metadata": {},
   "source": [
    "#### Use my devops json data to answer related questions.\n",
    "#### https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints\n",
    "#### import modules/libraries from rquirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad7929d2-6f69-4194-aaa8-593e3dbf7874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./myenv3.12/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: langchain==0.3.15 in ./myenv3.12/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.3.15)\n",
      "Requirement already satisfied: openai==1.60.0 in ./myenv3.12/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.60.0)\n",
      "Requirement already satisfied: jq==1.8.0 in ./myenv3.12/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.8.0)\n",
      "Requirement already satisfied: langchain-openai in ./myenv3.12/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (0.3.6)\n",
      "Requirement already satisfied: chromadb in ./myenv3.12/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.6.3)\n",
      "Requirement already satisfied: lark in ./myenv3.12/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
      "Requirement already satisfied: tiktoken in ./myenv3.12/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./myenv3.12/lib/python3.12/site-packages (from langchain==0.3.15->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./myenv3.12/lib/python3.12/site-packages (from langchain==0.3.15->-r requirements.txt (line 2)) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./myenv3.12/lib/python3.12/site-packages (from langchain==0.3.15->-r requirements.txt (line 2)) (3.11.11)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.31 in ./myenv3.12/lib/python3.12/site-packages (from langchain==0.3.15->-r requirements.txt (line 2)) (0.3.35)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in ./myenv3.12/lib/python3.12/site-packages (from langchain==0.3.15->-r requirements.txt (line 2)) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./myenv3.12/lib/python3.12/site-packages (from langchain==0.3.15->-r requirements.txt (line 2)) (0.3.1)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in ./myenv3.12/lib/python3.12/site-packages (from langchain==0.3.15->-r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./myenv3.12/lib/python3.12/site-packages (from langchain==0.3.15->-r requirements.txt (line 2)) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in ./myenv3.12/lib/python3.12/site-packages (from langchain==0.3.15->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./myenv3.12/lib/python3.12/site-packages (from langchain==0.3.15->-r requirements.txt (line 2)) (9.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./myenv3.12/lib/python3.12/site-packages (from openai==1.60.0->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./myenv3.12/lib/python3.12/site-packages (from openai==1.60.0->-r requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./myenv3.12/lib/python3.12/site-packages (from openai==1.60.0->-r requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./myenv3.12/lib/python3.12/site-packages (from openai==1.60.0->-r requirements.txt (line 3)) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./myenv3.12/lib/python3.12/site-packages (from openai==1.60.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./myenv3.12/lib/python3.12/site-packages (from openai==1.60.0->-r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./myenv3.12/lib/python3.12/site-packages (from openai==1.60.0->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: filelock in ./myenv3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./myenv3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./myenv3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./myenv3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./myenv3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./myenv3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./myenv3.12/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./myenv3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: packaging in ./myenv3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: build>=1.0.3 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (0.115.8)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./myenv3.12/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6)) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (3.11.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (1.29.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (1.70.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (32.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (3.10.15)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./myenv3.12/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 6)) (13.9.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./myenv3.12/lib/python3.12/site-packages (from tiktoken->-r requirements.txt (line 8)) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 2)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 2)) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 2)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 2)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./myenv3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 2)) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./myenv3.12/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai==1.60.0->-r requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: pyproject_hooks in ./myenv3.12/lib/python3.12/site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in ./myenv3.12/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 6)) (0.45.3)\n",
      "Requirement already satisfied: certifi in ./myenv3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai==1.60.0->-r requirements.txt (line 3)) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./myenv3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai==1.60.0->-r requirements.txt (line 3)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./myenv3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.60.0->-r requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./myenv3.12/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./myenv3.12/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./myenv3.12/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./myenv3.12/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./myenv3.12/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./myenv3.12/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./myenv3.12/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./myenv3.12/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./myenv3.12/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.31->langchain==0.3.15->-r requirements.txt (line 2)) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./myenv3.12/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.15->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./myenv3.12/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.15->-r requirements.txt (line 2)) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in ./myenv3.12/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 6)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./myenv3.12/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 6)) (25.1.24)\n",
      "Requirement already satisfied: protobuf in ./myenv3.12/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 6)) (5.29.3)\n",
      "Requirement already satisfied: sympy in ./myenv3.12/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 6)) (1.13.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./myenv3.12/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 6)) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in ./myenv3.12/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 6)) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./myenv3.12/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 6)) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in ./myenv3.12/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 6)) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.29.0 in ./myenv3.12/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 6)) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.50b0 in ./myenv3.12/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6)) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.50b0 in ./myenv3.12/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6)) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in ./myenv3.12/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6)) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.50b0 in ./myenv3.12/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6)) (0.50b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./myenv3.12/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6)) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in ./myenv3.12/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6)) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./myenv3.12/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 6)) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./myenv3.12/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 6)) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv3.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.15->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./myenv3.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.15->-r requirements.txt (line 2)) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv3.12/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.3.15->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./myenv3.12/lib/python3.12/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./myenv3.12/lib/python3.12/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 6)) (2.19.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./myenv3.12/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.15->-r requirements.txt (line 2)) (3.1.1)\n",
      "Requirement already satisfied: click>=8.0.0 in ./myenv3.12/lib/python3.12/site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 6)) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./myenv3.12/lib/python3.12/site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 6)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./myenv3.12/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6)) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./myenv3.12/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./myenv3.12/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./myenv3.12/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6)) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in ./myenv3.12/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6)) (14.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv3.12/lib/python3.12/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv3.12/lib/python3.12/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./myenv3.12/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./myenv3.12/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./myenv3.12/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (4.9)\n",
      "Requirement already satisfied: zipp>=3.20 in ./myenv3.12/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 6)) (3.21.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./myenv3.12/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.31->langchain==0.3.15->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./myenv3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 6)) (0.1.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./myenv3.12/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 6)) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv3.12/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./myenv3.12/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (0.6.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/badalsingh/Workspace/LLMs/LLMOps/myenv3.12/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edd3adf3-0bd4-4c42-8312-7b24c88906bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import sys\n",
    "\n",
    "from langchain.document_loaders import JSONLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53581d11-0b1c-475c-a621-3d999b35630e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '/Users/badalsingh/Workspace/LLMs/LLMOps/devops_data50.json', 'seq_num': 1}\n",
      "[\"Question: My build process packages my application in a .nupkg instead of a .zip. \\nAssuming my .nupkg contains a correctly-implemented appspec.yml and is otherwise appropriately bundled, can my deployment revision use it?\\n Answer:  Yes, your deployment revision can use a `.nupkg` file, provided that it contains a correctly-implemented `appspec.yml` file and is otherwise appropriately packaged according to the deployment service's requirements. \\n\\nFor example, if you are using AWS CodeDeploy, the `.nupkg` file needs to include the `appspec.yml` file, which defines how the application should be deployed, along with any other files needed for the deployment. The key is that the deployment service must support the type of package you are using, and you have to ensure that the contents of the `.nupkg` are correctly structured for deployment.\\n\\nMake sure to test the deployment with a small revision to verify that everything functions as expected. Additionally, you may want to consult the documentation of your deployment service to confirm that it supports `.nupkg` files and to see if there are any specific requirements or limitations.\", \"Question: So here is your job role:\\n\\nYou help in designing common modules that allow other teams to easily deploy their code onto production. This includes Java code, dockerfiles, ansible modules, build pipeline configurations, etc.\\nYou help promote DevOps practices for both other developer teams and the operation teams\\nYou are partially responsible for the staging environments\\nYou are responsible for investigating technologies that help the applications scale better and be more reliable\\n\\nHow would you identify your team? How would you identify yourself? You know that you could cheat and include DevOps in your name, but that would be wrong. On the other hand you want to be more specific, so a simple Software Engineer won't cut it, especially since you want the annoying recruiters to contact you for jobs related to this practice, not just because you like it, but because you know the average salary is also higher.\\n Answer:  Based on the job role you've described, a good title for your team could be \\\"Site Reliability Engineering (SRE) Team\\\" or \\\"Platform Engineering Team.\\\" Both titles convey a focus on reliability, scalability, and the operational aspects of software delivery while distinguishing the team from traditional DevOps roles.\\n\\nFor your personal title, consider using \\\"Platform Engineer\\\" or \\\"Site Reliability Engineer.\\\" Both titles clearly articulate your responsibilities and expertise in designing common modules, promoting DevOps practices, managing staging environments, and investigating scaling technologies. \\n\\nYou could also consider more specialized titles such as:\\n\\n- **DevOps Architect**: This highlights your role in designing solutions and promoting best practices across teams.\\n- **Cloud Infrastructure Engineer**: If your focus is primarily on cloud technologies and scaling applications in the cloud.\\n- **Automation Engineer**: If your role heavily emphasizes automation in deployment, configuration, and scaling.\\n\\nUsing these titles can help attract recruiters looking for candidates with your specific skill set while avoiding the generic \\\"Software Engineer\\\" title that may not reflect your specialized focus.\", \"Question: Amazon S3 has an option of cross-region replication which should be pretty fault-tolerant against region/zone outages.\\nDoes that mean those who are ranting about the outage did not make use of this aspect?\\nOr is that cross-region replication is not completely fool-proof and would not have helped?\\n Answer:  Amazon S3's cross-region replication (CRR) is indeed a powerful feature that allows for data to be automatically replicated across different AWS regions. This can enhance fault tolerance and data availability, especially in the case of regional outages. However, there are several considerations to keep in mind regarding its effectiveness during outages:\\n\\n1. **Configuration**: Users need to enable and correctly configure cross-region replication. If an organization does not have CRR set up, they would not benefit from this feature during an outage.\\n\\n2. **Replication Lag**: CRR is not instantaneous. There can be replication lag, meaning that any changes made to data may not be immediately reflected in the replicated region. If an outage occurs before data is replicated, the most recent changes could be lost.\\n\\n3. **Data Consistency**: Even with CRR, there could be scenarios where data consistency might be an issue if applications rely on the most up-to-date data being available at all times.\\n\\n4. **Cost**: Using cross-region replication incurs additional costs for data transfer and storage. Organizations may choose not to implement it due to budget constraints.\\n\\n5. **Different Failures**: While CRR provides protection against regional outages, other types of failures (like application-level failures or user erro\n"
     ]
    }
   ],
   "source": [
    "json_loader = JSONLoader(file_path=\"devops_data50.json\", jq_schema=\".\",text_content=False)\n",
    "json_loaded_data=json_loader.load()\n",
    "## Read metdata from JSonLoader of langchain\n",
    "d=json_loaded_data[0]\n",
    "print(d.metadata)\n",
    "## print content of this page loaded\n",
    "print(json_loaded_data[0].page_content[0:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e54b4d-9c70-428a-98ea-35dcfe5e168f",
   "metadata": {},
   "source": [
    "## 3rd stage\n",
    "#### Split document into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81bfab-f974-4400-aa01-20881bbc211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing I am using recursive text splitter, there are other options as well in langchain :\n",
    "### https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6392684-690b-446b-8efd-b6246cf79e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a283bb53-4d02-4252-b4fa-4f62b14f048d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My name is somthing you won't remember, until you know this.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test recursive text splitter with chunk size and chunk overlap\n",
    "chunk_size=2500\n",
    "chunk_overlap=250\n",
    "sample_text = \"My name is somthing you won't remember, until you know this.\"\n",
    "r_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "r_splitter.split_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d6c00cf-3968-42c0-bbb1-7e81e5b5fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter_new = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap,separators=[\"Question:\",\"\\n\\n\",\"\\n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aceac31f-3549-4823-acf3-29f01a58a8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Split the data from json file created in stage 2 above.\n",
    "my_data = r_splitter_new.split_documents(json_loaded_data)\n",
    "len(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c9b90-920f-40da-a297-41e4ce9126ba",
   "metadata": {},
   "source": [
    "## 4th stage\n",
    "### Vectorization and embedding\n",
    "### All chunks of data we obtained from splitting should be indexed , so that we can use the data to answer questions.\n",
    "### to perform this we will use embedding and vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "724b09ca-1450-4d35-89cc-810c5120de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings.azure_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48cbb3aa-4b55-463b-a2c6-0ef30aefc53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AZURE_OPENAI_ENDPOINT=https://ai-myraghub246415217390.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15\n",
      "env: AZURE_OPENAI_API_KEY=B33zpvwBj9Y4Lk4PF8RDZpNnsBA6vsKmHpyH376vcYPfJH84x4VFJQQJ99BBACHYHv6XJ3w3AAAAACOGILfZ\n"
     ]
    }
   ],
   "source": [
    "%env AZURE_OPENAI_ENDPOINT=https://ai-myraghub....embeddings?api-version=2023-05-15\n",
    "%env AZURE_OPENAI_API_KEY=B33...ILfZ\n",
    "az_openai_embedding = AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "075c1c95-20a6-4305-95a9-a823d9377f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai_embedding = OpenAIEmbeddings()\n",
    "az_openai_embedding = AzureOpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594d293-5f1b-4227-aa08-7d76aa22a17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "648c5568-db84-437b-bd53-8b66e1c311ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9424148941143409\n",
      "0.7456183639607952\n",
      "0.7402937622251249\n"
     ]
    }
   ],
   "source": [
    "### Sample embeddings\n",
    "# sample texts for embedding and embedding comparisions\n",
    "s1=\"I like fruits.\"\n",
    "s2=\"I like apples.\"\n",
    "s3=\"Sun rises from the east.\"\n",
    "emb1 = az_openai_embedding.embed_query(s1)\n",
    "emb2 = az_openai_embedding.embed_query(s2)\n",
    "emb3 = az_openai_embedding.embed_query(s3)\n",
    "import numpy as np\n",
    "### compare likeliness of embeddings\n",
    "print(np.dot(emb1,emb2))\n",
    "print(np.dot(emb1,emb3))\n",
    "print(np.dot(emb2,emb3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a5d6d2b-3561-413f-afd7-a6a6b2ad90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd8380eb-6916-4840-98ac-7aa895f06e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = \"./docs/chroma/\"\n",
    "!rm -rf ./docs/chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9c083f8-af42-4d6e-9f83-d62b6c9221f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=my_data, embedding=az_openai_embedding, persist_directory=db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "debdd727-ea1f-4432-b678-884f6920bdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e10d1087-02c9-4c44-bfe6-a1b93c78fc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Amazon S3 has an option of cross-region replication which should be pretty fault-tolerant against region/zone outages.\\nDoes that mean those who are ranting about the outage did not make use of this aspect?\\nOr is that cross-region replication is not completely fool-proof and would not have helped?\\n Answer:  Amazon S3's cross-region replication (CRR) is indeed a powerful feature that allows for data to be automatically replicated across different AWS regions. This can enhance fault tolerance and data availability, especially in the case of regional outages. However, there are several considerations to keep in mind regarding its effectiveness during outages:\\n\\n1. **Configuration**: Users need to enable and correctly configure cross-region replication. If an organization does not have CRR set up, they would not benefit from this feature during an outage.\\n\\n2. **Replication Lag**: CRR is not instantaneous. There can be replication lag, meaning that any changes made to data may not be immediately reflected in the replicated region. If an outage occurs before data is replicated, the most recent changes could be lost.\\n\\n3. **Data Consistency**: Even with CRR, there could be scenarios where data consistency might be an issue if applications rely on the most up-to-date data being available at all times.\\n\\n4. **Cost**: Using cross-region replication incurs additional costs for data transfer and storage. Organizations may choose not to implement it due to budget constraints.\\n\\n5. **Different Failures**: While CRR provides protection against regional outages, other types of failures (like application-level failures or user errors) could still affect data. Organizations should implement comprehensive backup strategies alongside replication.\\n\\n6. **RPO and RTO**: Organizations should consider their Recovery Point Objectives (RPO) and Recovery Time Objectives (RTO) when designing their architecture. CRR may not meet all requirements for certain applications.\\n\\nIn conclusion, while cross-region replication can significantly enhance fault tolerance, it is not fool-proof and may not fully mitigate all issues during an outage. Organizations that experienced issues during an outage may have either not implemented CRR, or they may have been affected by other factors (like replication lag or application-level problems) that CRR does not address.\", \"\n"
     ]
    }
   ],
   "source": [
    "docs=vectordb.similarity_search(\"Amazon S3 has an option of cross-region replication\",k=2)\n",
    "len(docs)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4f035-953f-40f5-a943-c5e5af730db0",
   "metadata": {},
   "source": [
    "### Now time to use Chat GPT LLM model to use for question anwering\n",
    "### First setup envrionment vraibles for AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT,OPENAI_API_VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fc360718-9e3c-40b1-97c1-f2a959e269e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AZURE_OPENAI_ENDPOINT=https://ai-myraghub246415217390.openai.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview\n",
      "env: AZURE_OPENAI_API_KEY=B33zpvwBj9Y4Lk4PF8RDZpNnsBA6vsKmHpyH376vcYPfJH84x4VFJQQJ99BBACHYHv6XJ3w3AAAAACOGILfZ\n",
      "env: OPENAI_API_VERSION=2025-01-01-preview\n",
      "env: AZURE_OPENAI_DEPLOYMENT_NAME=gpt-35-turbo\n"
     ]
    }
   ],
   "source": [
    "%env AZURE_OPENAI_ENDPOINT=https://ai-myrag....api-version=2025-01-01-preview\n",
    "%env AZURE_OPENAI_API_KEY=B33...LfZ\n",
    "%env OPENAI_API_VERSION=2025-01-01-preview\n",
    "%env AZURE_OPENAI_DEPLOYMENT_NAME=gpt-35-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5da057df-854a-4863-a8cc-7549ec5919b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import AzureOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "#from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "#from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5d0124e0-72fe-49c5-9065-4918c429b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(temperature=0,name=\"gpt-35-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3dc3fe2f-fa92-49d0-84fb-4a8fad144a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.chains import RetrievalQA\n",
    "from langchain.chains import (create_retrieval_chain,create_history_aware_retriever)\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.mapreduce import MapReduceDocumentsChain\n",
    "from langchain_core.prompts import (ChatPromptTemplate, MessagesPlaceholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6f40c978-caf0-41f4-8b41-74c6dc0d8f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 11, 'total_tokens': 27, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_0165350fbb', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-4b9109a0-3764-4acd-8108-bc152393dac7-0', usage_metadata={'input_tokens': 11, 'output_tokens': 16, 'total_tokens': 27, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a3582778-14f8-4087-8b83-55954225dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The document contains information about managing data in Amazon S3 (Simple Storage Service) within the context of various use cases and best practices. It covers topics such as HIPAA compliance considerations, strategies for backing up data from S3, ensuring site availability during S3 outages, and securely managing secrets in serverless applications using AWS Lambda. The document provides detailed guidance on encryption, access controls, redundancy, diversification, and fallback mechanisms to enhance the security and reliability of applications utilizing S3.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#llm.invoke(\"Tell me a joke\")\n",
    "#Stuff technique\n",
    "#need to check other 3 techniques like Map_reduce, Refine and map_rerank\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "print(vectordb._collection.count())\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retriever=vectordb.as_retriever()\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "query=\"what info does the doc contain regarding aws s3.\"\n",
    "chain.invoke({\"input\": query})[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f66e0834-c0cf-407f-b869-35fa44a845a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/badalsingh/Workspace/LLMs/LLMOps/myenv3.12/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b5e9eb1e-3c6c-4cf6-a55d-4be8d4e933ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "#question_answer_chain_chat = create_stuff_documents_chain(llm, prompt,memory)\n",
    "\n",
    "\n",
    "### https://python.langchain.com/api_reference/langchain/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html\n",
    "\n",
    "### Contex\n",
    "# Contextualize question\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, just \"\n",
    "    \"reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Answer question\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the following pieces of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, just say that you don't know.\" \n",
    "    \"Use three sentences maximum and keep the answer concise.\"\n",
    "    \"Context: {context}\"\n",
    ") \n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt) \n",
    "\n",
    "rag_chain = create_retrieval_chain(\n",
    "    history_aware_retriever, question_answer_chain\n",
    ")\n",
    "#chat_history = []\n",
    "#query=\"what info does the doc contain regarding aws s3.\"\n",
    "#rag_chain.invoke({\"input\":query,\"chat_history\":chat_history})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "66000f8f-b12a-4fd1-9bd2-5184eb3c21d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document contains information about managing data in Amazon S3, including strategies for handling HIPAA-compliant data, ordering physical backups using AWS Snowball, ensuring site availability during S3 outages, and securely managing secrets in AWS Lambda functions without committing them to source control. It also discusses best practices for encryption, access controls, monitoring, and redundancy when working with S3 in AWS architectures.\n",
      "{'input': 'what strategies it mentions to backup data?', 'chat_history': [HumanMessage(content='how to package my application in a .nupkg', additional_kwargs={}, response_metadata={}), 'The document contains information about managing data in Amazon S3, including strategies for handling HIPAA-compliant data, ordering physical backups using AWS Snowball, ensuring site availability during S3 outages, and securely managing secrets in AWS Lambda functions without committing them to source control. It also discusses best practices for encryption, access controls, monitoring, and redundancy when working with S3 in AWS architectures.'], 'context': [Document(metadata={'seq_num': 1, 'source': '/Users/badalsingh/Workspace/LLMs/LLMOps/devops_data50.json'}, page_content='Question: What is a good strategy to keep my site online when S3 goes offline?\\\\nIf S3 US East 1 goes offline, how should I have my app configured/structured to prevent that taking my entire site offline?\\\\nWhat are the best strategies to diversify in this sort of situation?\\\\n Answer:  To ensure your site remains online even if Amazon S3 (Simple Storage Service) goes offline\\\\u2014particularly in the US East (N. Virginia) region\\\\u2014it\\'s important to implement a multi-faceted strategy that includes redundancy, diversification, and fallback mechanisms. Here are some best practices and strategies to consider:\\\\n\\\\n### 1. Use Cross-Region Replication\\\\n- **Set Up Cross-Region Replication**: Configure S3 bucket replication to replicate your data to a different AWS region (e.g., US West, Europe, or Asia). This way, if the US East region experiences downtime, you can serve static content from a replicated bucket in another region.\\\\n\\\\n### 2. Content Delivery Network (CDN)\\\\n- **Utilize a CDN**: Use a CDN (like Amazon CloudFront, Cloudflare, or Akamai) to cache and serve your static assets. CDNs replicate your content across multiple edge locations, providing resilience against S3 outages in a specific region. Make sure your CDN is configured to fetch from multiple origins if necessary.\\\\n\\\\n### 3. Multi-Cloud Strategy\\\\n- **Diversify Storage Solutions**: Consider using multiple cloud storage providers (e.g., Google Cloud Storage, Microsoft Azure Blob Storage) to store your assets. This way, if one service goes down, you have others available to serve your content.\\\\n\\\\n### 4. Local Caching\\\\n- **Implement Local Caching**: Cache static assets on your web servers or in-memory caches (like Redis or Memcached). This minimizes the dependency on S3 by allowing your application to serve cached content in case of S3 outages.\\\\n\\\\n### 5. Fallback Mechanisms\\\\n- **Fallback Logic in Your Application**: Implement logic in your application to handle failures gracefully. For example, if your app can\\'t retrieve an asset from S3, it can attempt to retrieve it from a cached version or another region or service.\\\\n\\\\n### 6. Backup and Restore Solutions\\\\n- **Regular Backups**: Regularly backup your S3 data to a local file system or another cloud provider. This allows you to restore your application data quickly in case of a major outage.\\\\n\\\\n### 7. Monitoring and Alerts\\\\n- **Implement Monitoring and Alerts**: Use monitoring tools to keep an eye on your S3 availability and performance. Set up alerts to notify your team if S3 service in your region is experiencing issues, so you can take action quickly.\\\\n\\\\n### 8. Infrastructure as Code (IaC)\\\\n- **Use IaC for Quick Recovery**: Use tools like Terraform or AWS CloudFormation to manage your infrastructure. In the event of an S3 outage, you can quickly reconfigure your application to point to an alternative storage solution or region.\\\\n\\\\n### 9. Load Balancing and Failover Strategies\\\\n- **Load Balancers**: If you have a distributed architecture, use load balancers to route requests to different regions or services based on availability. \\\\n\\\\n### 10. Regular Testing and Updates\\\\n- **Test Your Failover Strategy**: Regularly simulate S3 outages and test your application\\\\u2019s failover mechanisms. This will help you ensure your strategies work as intended during an actual outage.\\\\n\\\\n### Conclusion\\\\nBy implementing these strategies, you can significantly reduce the risk of downtime when S3 experiences issues, ensuring that your site remains functional and available to users. A combination of cross-region replication, CDN usage, local caching, multi-cloud strategies, and robust monitoring will help create a resilient architecture.\", \"'), Document(metadata={'seq_num': 1, 'source': '/Users/badalsingh/Workspace/LLMs/LLMOps/devops_data50.json'}, page_content='Question: I\\'d like to order backup of data to be copied out from S3 to a physical drive (SATA HD) and to be shipped to the address. I know it\\'s possible using Amazon Glacier, however I cannot find such option anywhere. What would be the easiest way to do that?\\\\n Answer:  To order a physical copy of your data stored in Amazon S3 and have it shipped to you, you would typically use a service called **AWS Snowball** rather than Amazon Glacier. Snowball is a petabyte-scale data transfer service that allows you to securely transfer large amounts of data into and out of AWS using physical devices. Here are the steps to follow:\\\\n\\\\n1. **Set Up AWS Snowball:**\\\\n   - Log into your AWS Management Console.\\\\n   - Navigate to the AWS Snowball service.\\\\n   - Create a job by selecting the option to export data from S3.\\\\n   - Choose the data you want to transfer and the destination for the Snowball device (the physical drive).\\\\n\\\\n2. **Choose Job Type:**\\\\n   - When creating your job, you can specify whether you want to export data from S3 to the Snowball device. Select the S3 bucket that contains the data you want to copy.\\\\n\\\\n3. **Shipping Information:**\\\\n   - Enter your shipping address. AWS will ship the Snowball device to your specified address.\\\\n\\\\n4. **Receive the Snowball Device:**\\\\n   - Once you receive the Snowball device, follow the instructions included with it to connect it to your local network.\\\\n\\\\n5. **Transfer Data:**\\\\n   - The Snowball device will come with software that allows you to copy your data from the S3 bucket to the device.\\\\n\\\\n6. **Return the Device:**\\\\n   - After the data transfer is complete, return the Snowball device to AWS using the prepaid shipping label included with it.\\\\n\\\\n7. **Data Availability:**\\\\n   - Once AWS receives the Snowball device, they will process the data and send you a physical copy (or you can choose to have it uploaded to an external drive, depending on your selection during the job setup).\\\\n\\\\n**Additional Considerations:**\\\\n- **Pricing:** Be aware of the costs associated with AWS Snowball. You will incur charges for the device usage as well as data transfer.\\\\n- **Data Encryption:** Data transferred via Snowball is encrypted, which helps ensure your data\\'s security during transport.\\\\n\\\\n### Alternatives:\\\\nIf you\\'re looking to backup smaller amounts of data, you can simply download the data from S3 directly to your local drive. For larger datasets, AWS Snowball is usually the most efficient method.\\\\n\\\\nPlease ensure to check the AWS documentation for the latest options and instructions, as services and features may change.\", \"'), Document(metadata={'seq_num': 1, 'source': '/Users/badalsingh/Workspace/LLMs/LLMOps/devops_data50.json'}, page_content='Question: What happens when one of the storage bricks goes down (HDD failure), how does GlusterFS deal with data recover? Is the stored data still safe?\\\\n Answer:  When a storage brick in a GlusterFS volume goes down due to an HDD failure or any other issue, GlusterFS has mechanisms in place to handle data recovery and ensure data integrity, provided that you have set up the system appropriately.\\\\n\\\\n### Key Concepts:\\\\n\\\\n1. **Replication**: If you have configured your GlusterFS volume with replication (e.g., a replicated volume with at least two bricks), each piece of data is stored on multiple bricks. When one brick fails, the other brick(s) still hold copies of the data. In this case, the stored data remains safe, and you can continue to read and write to the volume without disruption.\\\\n\\\\n2. **Redundancy**: Using replicated volumes provides redundancy. If one brick fails, GlusterFS will still have access to the data from the other brick(s), allowing continued access.\\\\n\\\\n3. **Self-Healing**: When a failed brick is repaired or replaced, GlusterFS uses self-healing mechanisms to synchronize the data across the bricks. This means it will automatically copy the missing or out-of-date data from the remaining healthy bricks to the repaired or replaced brick.\\\\n\\\\n4. **Distributed Volumes**: If you are using a distributed volume (where data is spread across different bricks), the loss of a single brick could result in data loss, depending on how the data is distributed. However, this setup does not provide redundancy, so it is critical to use replication or some other form of backup to ensure data safety.\\\\n\\\\n5. **Monitoring and Alerts**: GlusterFS has monitoring capabilities that allow administrators to track the health of the bricks. It\\'s crucial to have alerts set up to respond to brick failures quickly, minimizing data risk.\\\\n\\\\n6. **Recovery Process**: When a brick goes down, you will see warnings in the GlusterFS logs. The volume status can be checked using `gluster volume status` to identify which bricks are down. If the volume is replicated, the other bricks will continue to serve data. Once the failed brick is brought back online or replaced, you can initiate a self-heal operation to restore data consistency.\\\\n\\\\n### Conclusion\\\\n\\\\nIf your GlusterFS setup uses replication and redundancy correctly, the data should be safe even when a storage brick goes down. Regular monitoring and management are essential to ensure that all bricks remain healthy and that any failed components are promptly addressed. Always ensure you have backups as part of your data protection strategy, as hardware failures can happen and replication alone might not safeguard against all potential issues, such as human error or data corruption.\", \"'), Document(metadata={'seq_num': 1, 'source': '/Users/badalsingh/Workspace/LLMs/LLMOps/devops_data50.json'}, page_content='Question: Amazon S3 has an option of cross-region replication which should be pretty fault-tolerant against region/zone outages.\\\\nDoes that mean those who are ranting about the outage did not make use of this aspect?\\\\nOr is that cross-region replication is not completely fool-proof and would not have helped?\\\\n Answer:  Amazon S3\\'s cross-region replication (CRR) is indeed a powerful feature that allows for data to be automatically replicated across different AWS regions. This can enhance fault tolerance and data availability, especially in the case of regional outages. However, there are several considerations to keep in mind regarding its effectiveness during outages:\\\\n\\\\n1. **Configuration**: Users need to enable and correctly configure cross-region replication. If an organization does not have CRR set up, they would not benefit from this feature during an outage.\\\\n\\\\n2. **Replication Lag**: CRR is not instantaneous. There can be replication lag, meaning that any changes made to data may not be immediately reflected in the replicated region. If an outage occurs before data is replicated, the most recent changes could be lost.\\\\n\\\\n3. **Data Consistency**: Even with CRR, there could be scenarios where data consistency might be an issue if applications rely on the most up-to-date data being available at all times.\\\\n\\\\n4. **Cost**: Using cross-region replication incurs additional costs for data transfer and storage. Organizations may choose not to implement it due to budget constraints.\\\\n\\\\n5. **Different Failures**: While CRR provides protection against regional outages, other types of failures (like application-level failures or user errors) could still affect data. Organizations should implement comprehensive backup strategies alongside replication.\\\\n\\\\n6. **RPO and RTO**: Organizations should consider their Recovery Point Objectives (RPO) and Recovery Time Objectives (RTO) when designing their architecture. CRR may not meet all requirements for certain applications.\\\\n\\\\nIn conclusion, while cross-region replication can significantly enhance fault tolerance, it is not fool-proof and may not fully mitigate all issues during an outage. Organizations that experienced issues during an outage may have either not implemented CRR, or they may have been affected by other factors (like replication lag or application-level problems) that CRR does not address.\", \"')], 'answer': 'The document discusses ordering physical backups using AWS Snowball as a strategy to backup data stored in Amazon S3. AWS Snowball is a service that allows for secure transfer of large amounts of data into and out of AWS using physical devices. This method is particularly useful for handling large datasets efficiently.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "query=\"what info does the doc contain regarding aws s3.\"\n",
    "resp1 = rag_chain.invoke({\"input\":query,\"chat_history\":chat_history})\n",
    "print(resp1[\"answer\"])\n",
    "\n",
    "chat_history.extend([HumanMessage(content=question), resp1[\"answer\"]])\n",
    "\n",
    "second_question = \"what strategies it mentions to backup data?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "140ec993-aa0a-4665-a751-08eb0dab5d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what strategies it mentions to backup data.',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'seq_num': 1, 'source': '/Users/badalsingh/Workspace/LLMs/LLMOps/devops_data50.json'}, page_content='Question: What is a good strategy to keep my site online when S3 goes offline?\\\\nIf S3 US East 1 goes offline, how should I have my app configured/structured to prevent that taking my entire site offline?\\\\nWhat are the best strategies to diversify in this sort of situation?\\\\n Answer:  To ensure your site remains online even if Amazon S3 (Simple Storage Service) goes offline\\\\u2014particularly in the US East (N. Virginia) region\\\\u2014it\\'s important to implement a multi-faceted strategy that includes redundancy, diversification, and fallback mechanisms. Here are some best practices and strategies to consider:\\\\n\\\\n### 1. Use Cross-Region Replication\\\\n- **Set Up Cross-Region Replication**: Configure S3 bucket replication to replicate your data to a different AWS region (e.g., US West, Europe, or Asia). This way, if the US East region experiences downtime, you can serve static content from a replicated bucket in another region.\\\\n\\\\n### 2. Content Delivery Network (CDN)\\\\n- **Utilize a CDN**: Use a CDN (like Amazon CloudFront, Cloudflare, or Akamai) to cache and serve your static assets. CDNs replicate your content across multiple edge locations, providing resilience against S3 outages in a specific region. Make sure your CDN is configured to fetch from multiple origins if necessary.\\\\n\\\\n### 3. Multi-Cloud Strategy\\\\n- **Diversify Storage Solutions**: Consider using multiple cloud storage providers (e.g., Google Cloud Storage, Microsoft Azure Blob Storage) to store your assets. This way, if one service goes down, you have others available to serve your content.\\\\n\\\\n### 4. Local Caching\\\\n- **Implement Local Caching**: Cache static assets on your web servers or in-memory caches (like Redis or Memcached). This minimizes the dependency on S3 by allowing your application to serve cached content in case of S3 outages.\\\\n\\\\n### 5. Fallback Mechanisms\\\\n- **Fallback Logic in Your Application**: Implement logic in your application to handle failures gracefully. For example, if your app can\\'t retrieve an asset from S3, it can attempt to retrieve it from a cached version or another region or service.\\\\n\\\\n### 6. Backup and Restore Solutions\\\\n- **Regular Backups**: Regularly backup your S3 data to a local file system or another cloud provider. This allows you to restore your application data quickly in case of a major outage.\\\\n\\\\n### 7. Monitoring and Alerts\\\\n- **Implement Monitoring and Alerts**: Use monitoring tools to keep an eye on your S3 availability and performance. Set up alerts to notify your team if S3 service in your region is experiencing issues, so you can take action quickly.\\\\n\\\\n### 8. Infrastructure as Code (IaC)\\\\n- **Use IaC for Quick Recovery**: Use tools like Terraform or AWS CloudFormation to manage your infrastructure. In the event of an S3 outage, you can quickly reconfigure your application to point to an alternative storage solution or region.\\\\n\\\\n### 9. Load Balancing and Failover Strategies\\\\n- **Load Balancers**: If you have a distributed architecture, use load balancers to route requests to different regions or services based on availability. \\\\n\\\\n### 10. Regular Testing and Updates\\\\n- **Test Your Failover Strategy**: Regularly simulate S3 outages and test your application\\\\u2019s failover mechanisms. This will help you ensure your strategies work as intended during an actual outage.\\\\n\\\\n### Conclusion\\\\nBy implementing these strategies, you can significantly reduce the risk of downtime when S3 experiences issues, ensuring that your site remains functional and available to users. A combination of cross-region replication, CDN usage, local caching, multi-cloud strategies, and robust monitoring will help create a resilient architecture.\", \"'),\n",
       "  Document(metadata={'seq_num': 1, 'source': '/Users/badalsingh/Workspace/LLMs/LLMOps/devops_data50.json'}, page_content='Question: I\\'d like to order backup of data to be copied out from S3 to a physical drive (SATA HD) and to be shipped to the address. I know it\\'s possible using Amazon Glacier, however I cannot find such option anywhere. What would be the easiest way to do that?\\\\n Answer:  To order a physical copy of your data stored in Amazon S3 and have it shipped to you, you would typically use a service called **AWS Snowball** rather than Amazon Glacier. Snowball is a petabyte-scale data transfer service that allows you to securely transfer large amounts of data into and out of AWS using physical devices. Here are the steps to follow:\\\\n\\\\n1. **Set Up AWS Snowball:**\\\\n   - Log into your AWS Management Console.\\\\n   - Navigate to the AWS Snowball service.\\\\n   - Create a job by selecting the option to export data from S3.\\\\n   - Choose the data you want to transfer and the destination for the Snowball device (the physical drive).\\\\n\\\\n2. **Choose Job Type:**\\\\n   - When creating your job, you can specify whether you want to export data from S3 to the Snowball device. Select the S3 bucket that contains the data you want to copy.\\\\n\\\\n3. **Shipping Information:**\\\\n   - Enter your shipping address. AWS will ship the Snowball device to your specified address.\\\\n\\\\n4. **Receive the Snowball Device:**\\\\n   - Once you receive the Snowball device, follow the instructions included with it to connect it to your local network.\\\\n\\\\n5. **Transfer Data:**\\\\n   - The Snowball device will come with software that allows you to copy your data from the S3 bucket to the device.\\\\n\\\\n6. **Return the Device:**\\\\n   - After the data transfer is complete, return the Snowball device to AWS using the prepaid shipping label included with it.\\\\n\\\\n7. **Data Availability:**\\\\n   - Once AWS receives the Snowball device, they will process the data and send you a physical copy (or you can choose to have it uploaded to an external drive, depending on your selection during the job setup).\\\\n\\\\n**Additional Considerations:**\\\\n- **Pricing:** Be aware of the costs associated with AWS Snowball. You will incur charges for the device usage as well as data transfer.\\\\n- **Data Encryption:** Data transferred via Snowball is encrypted, which helps ensure your data\\'s security during transport.\\\\n\\\\n### Alternatives:\\\\nIf you\\'re looking to backup smaller amounts of data, you can simply download the data from S3 directly to your local drive. For larger datasets, AWS Snowball is usually the most efficient method.\\\\n\\\\nPlease ensure to check the AWS documentation for the latest options and instructions, as services and features may change.\", \"'),\n",
       "  Document(metadata={'seq_num': 1, 'source': '/Users/badalsingh/Workspace/LLMs/LLMOps/devops_data50.json'}, page_content='Question: What happens when one of the storage bricks goes down (HDD failure), how does GlusterFS deal with data recover? Is the stored data still safe?\\\\n Answer:  When a storage brick in a GlusterFS volume goes down due to an HDD failure or any other issue, GlusterFS has mechanisms in place to handle data recovery and ensure data integrity, provided that you have set up the system appropriately.\\\\n\\\\n### Key Concepts:\\\\n\\\\n1. **Replication**: If you have configured your GlusterFS volume with replication (e.g., a replicated volume with at least two bricks), each piece of data is stored on multiple bricks. When one brick fails, the other brick(s) still hold copies of the data. In this case, the stored data remains safe, and you can continue to read and write to the volume without disruption.\\\\n\\\\n2. **Redundancy**: Using replicated volumes provides redundancy. If one brick fails, GlusterFS will still have access to the data from the other brick(s), allowing continued access.\\\\n\\\\n3. **Self-Healing**: When a failed brick is repaired or replaced, GlusterFS uses self-healing mechanisms to synchronize the data across the bricks. This means it will automatically copy the missing or out-of-date data from the remaining healthy bricks to the repaired or replaced brick.\\\\n\\\\n4. **Distributed Volumes**: If you are using a distributed volume (where data is spread across different bricks), the loss of a single brick could result in data loss, depending on how the data is distributed. However, this setup does not provide redundancy, so it is critical to use replication or some other form of backup to ensure data safety.\\\\n\\\\n5. **Monitoring and Alerts**: GlusterFS has monitoring capabilities that allow administrators to track the health of the bricks. It\\'s crucial to have alerts set up to respond to brick failures quickly, minimizing data risk.\\\\n\\\\n6. **Recovery Process**: When a brick goes down, you will see warnings in the GlusterFS logs. The volume status can be checked using `gluster volume status` to identify which bricks are down. If the volume is replicated, the other bricks will continue to serve data. Once the failed brick is brought back online or replaced, you can initiate a self-heal operation to restore data consistency.\\\\n\\\\n### Conclusion\\\\n\\\\nIf your GlusterFS setup uses replication and redundancy correctly, the data should be safe even when a storage brick goes down. Regular monitoring and management are essential to ensure that all bricks remain healthy and that any failed components are promptly addressed. Always ensure you have backups as part of your data protection strategy, as hardware failures can happen and replication alone might not safeguard against all potential issues, such as human error or data corruption.\", \"'),\n",
       "  Document(metadata={'seq_num': 1, 'source': '/Users/badalsingh/Workspace/LLMs/LLMOps/devops_data50.json'}, page_content='Question: Amazon S3 has an option of cross-region replication which should be pretty fault-tolerant against region/zone outages.\\\\nDoes that mean those who are ranting about the outage did not make use of this aspect?\\\\nOr is that cross-region replication is not completely fool-proof and would not have helped?\\\\n Answer:  Amazon S3\\'s cross-region replication (CRR) is indeed a powerful feature that allows for data to be automatically replicated across different AWS regions. This can enhance fault tolerance and data availability, especially in the case of regional outages. However, there are several considerations to keep in mind regarding its effectiveness during outages:\\\\n\\\\n1. **Configuration**: Users need to enable and correctly configure cross-region replication. If an organization does not have CRR set up, they would not benefit from this feature during an outage.\\\\n\\\\n2. **Replication Lag**: CRR is not instantaneous. There can be replication lag, meaning that any changes made to data may not be immediately reflected in the replicated region. If an outage occurs before data is replicated, the most recent changes could be lost.\\\\n\\\\n3. **Data Consistency**: Even with CRR, there could be scenarios where data consistency might be an issue if applications rely on the most up-to-date data being available at all times.\\\\n\\\\n4. **Cost**: Using cross-region replication incurs additional costs for data transfer and storage. Organizations may choose not to implement it due to budget constraints.\\\\n\\\\n5. **Different Failures**: While CRR provides protection against regional outages, other types of failures (like application-level failures or user errors) could still affect data. Organizations should implement comprehensive backup strategies alongside replication.\\\\n\\\\n6. **RPO and RTO**: Organizations should consider their Recovery Point Objectives (RPO) and Recovery Time Objectives (RTO) when designing their architecture. CRR may not meet all requirements for certain applications.\\\\n\\\\nIn conclusion, while cross-region replication can significantly enhance fault tolerance, it is not fool-proof and may not fully mitigate all issues during an outage. Organizations that experienced issues during an outage may have either not implemented CRR, or they may have been affected by other factors (like replication lag or application-level problems) that CRR does not address.\", \"')],\n",
       " 'answer': 'The strategies mentioned to backup data include using Cross-Region Replication in Amazon S3, implementing Local Caching on web servers or in-memory caches, utilizing Backup and Restore Solutions for regular backups, and considering a Multi-Cloud Strategy with multiple cloud storage providers. These strategies aim to ensure data redundancy, availability, and resilience in case of outages or failures.'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"what strategies it mentions to backup data.\"\n",
    "rag_chain.invoke({\"input\":query,\"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bace527d-7d9b-4522-bd6b-41b060c77fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The strategies mentioned to backup data include using Cross-Region Replication in Amazon S3, implementing Local Caching on web servers or in-memory caches, utilizing Backup and Restore Solutions for regular backups, and considering a Multi-Cloud Strategy with multiple cloud storage providers. These strategies aim to ensure data redundancy, availability, and quick recovery in case of outages or failures.'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question=\"Amazon S3 has an option of cross-region replication\"\n",
    "#result = qa_chain({\"query\": question})\n",
    "#result[\"result\"]\n",
    "query=\"what strategies it mentions to backup data.\"\n",
    "chain.invoke({\"input\": query})[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "519362c2-54e9-4b35-bb16-0069613338e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langsmith in ./myenv3.12/lib/python3.12/site-packages (0.3.1)\n",
      "Collecting langsmith\n",
      "  Downloading langsmith-0.3.15-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./myenv3.12/lib/python3.12/site-packages (from langsmith) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./myenv3.12/lib/python3.12/site-packages (from langsmith) (3.10.15)\n",
      "Requirement already satisfied: packaging>=23.2 in ./myenv3.12/lib/python3.12/site-packages (from langsmith) (24.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./myenv3.12/lib/python3.12/site-packages (from langsmith) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in ./myenv3.12/lib/python3.12/site-packages (from langsmith) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./myenv3.12/lib/python3.12/site-packages (from langsmith) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./myenv3.12/lib/python3.12/site-packages (from langsmith) (0.23.0)\n",
      "Requirement already satisfied: anyio in ./myenv3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./myenv3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./myenv3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.7)\n",
      "Requirement already satisfied: idna in ./myenv3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./myenv3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv3.12/lib/python3.12/site-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./myenv3.12/lib/python3.12/site-packages (from pydantic<3,>=1->langsmith) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./myenv3.12/lib/python3.12/site-packages (from pydantic<3,>=1->langsmith) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv3.12/lib/python3.12/site-packages (from requests<3,>=2->langsmith) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv3.12/lib/python3.12/site-packages (from requests<3,>=2->langsmith) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./myenv3.12/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
      "Downloading langsmith-0.3.15-py3-none-any.whl (343 kB)\n",
      "Installing collected packages: langsmith\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.3.1\n",
      "    Uninstalling langsmith-0.3.1:\n",
      "      Successfully uninstalled langsmith-0.3.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.3.16 requires langchain<0.4.0,>=0.3.16, but you have langchain 0.3.15 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langsmith-0.3.15\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/badalsingh/Workspace/LLMs/LLMOps/myenv3.12/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7695c053-784d-485c-aeab-ae78a96210fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Langsmith\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls...0\" # replace dots with your api key\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"myllmproject1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"B33...LfZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ebd4b2d9-c25f-470e-b823-fc5289d18632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a356c6c0-7193-439c-8103-f8324853cf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B33zpvwBj9Y4Lk4PF8RDZpNnsBA6vsKmHpyH376vcYPfJH84x4VFJQQJ99BBACHYHv6XJ3w3AAAAACOGILfZ\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a53989-9d87-4baf-94ab-566a1541765e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.12)",
   "language": "python",
   "name": "myenv3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
